bayes.accuracy
cat('\n#Naive Bayes F1-Score\n')
bayes.f1 <- (2 * bayes.precision * bayes.recall)/(bayes.precision + bayes.recall)
bayes.f1
WD.predbag <- predict.bagging(WD.bag, WD.test)
cat('\n#Bagging Confusion\n')
WD.predbag$confusion
cat('\n#Bagging Precision\n')
bag.precision <- WD.predbag$confusion[2, 2]/sum(WD.predbag$confusion[2, ])
bag.precision
cat('\n#Bagging Recall\n')
bag.recall <- WD.predbag$confusion[2, 2]/sum(WD.predbag$confusion[, 2])
bag.recall
cat('\n#Bagging Accuracy\n')
bag.accuracy <- sum(WD.predbag$confusion[2, 2], WD.predbag$confusion[1, 1])/
sum(WD.predbag$confusion)
bag.accuracy
cat('\n#Bagging F1-Score\n')
bag.f1 <- (2 * bag.precision * bag.recall)/(bag.precision + bag.recall)
bag.f1
WD.predboost <- predict.boosting(WD.boost, newdata = WD.test)
cat('\n#Boosting Confusion\n')
WD.predboost$confusion
cat('\n#Boosting Precision\n')
boost.precision <- WD.predboost$confusion[2, 2]/sum(WD.predboost$confusion[2, ])
boost.precision
cat('\n#Boosting Recall\n')
boost.recall <- WD.predboost$confusion[2, 2]/sum(WD.predboost$confusion[, 2])
boost.recall
cat('\n#Boosting Accuracy\n')
boost.accuracy <- sum(WD.predboost$confusion[2, 2], WD.predboost$confusion[1, 1])/
sum(WD.predboost$confusion)
boost.accuracy
cat('\n#Boosting F1-Score\n')
boost.f1 <- (2 * boost.precision * boost.recall)/(boost.precision + boost.recall)
boost.f1
WD.predrf <- predict(WD.forest, WD.test)
t3 = table(Predicted_Class = WD.predrf, Actual_Class = WD.test$Class)
cat('\n#Random Forest Confusion\n')
t3
cat('\n#Random Forest Precision\n')
forest.precision <- t3[2, 2]/sum(t3[2, ])
forest.precision
cat('\n#Random Forest Recall\n')
forest.recall <- t3[2, 2]/sum(t3[, 2])
forest.recall
cat('\n#Random Forest Accuracy\n')
forest.accuracy <- sum(t3[2, 2], t3[1, 1])/sum(t3)
forest.accuracy
cat('\n#Random Forest F1-Score\n')
forest.f1 <- (2 * forest.precision * forest.recall)/(forest.precision +
forest.recall)
forest.f1
WD.predtree.conf <- predict(WD.tree, WD.test, type = 'vector')
WDtreepred <- ROCR::prediction( WD.predtree.conf[, 2], WD.test$Class)
WDtreeperf <- performance(WDtreepred, 'tpr', 'fpr')
plot(WDtreeperf, col = 'orange')
abline(0,1)
WD.predbayes.conf <- predict(WD.bayes, WD.test, type = 'raw')
WDbayespred <- ROCR::prediction(WD.predbayes.conf[,2], WD.test$Class)
WDbayesperf <- performance(WDbayespred, 'tpr', 'fpr')
plot(WDbayesperf, add = TRUE, col = 'blueviolet')
WDbagpred <- ROCR::prediction(WD.predbag$prob[,2], WD.test$Class)
WDbagperf <- performance(WDbagpred, 'tpr', 'fpr')
plot(WDbagperf, add = TRUE, col = 'blue')
WDboostpred <- ROCR::prediction(WD.predboost$prob[,2], WD.test$Class)
WDboostperf <- performance(WDboostpred, 'tpr', 'fpr')
plot(WDboostperf, add = TRUE, col = 'red')
WD.predrf.conf <- predict(WD.forest, WD.test, type = 'prob')
WDrfpred <- ROCR::prediction(WD.predrf.conf[,2], WD.test$Class)
WDrfperf <- performance(WDrfpred, 'tpr', 'fpr')
plot(WDrfperf, add = TRUE, col = 'darkgreen')
legend('bottomright', legend = c('Decision Tree', 'Naive Bayes', 'Bagging', 'Boosting',
'Random Forest'), col = c('orange', 'blueviolet',
'blue', 'red', 'darkgreen'),
lty = 1, cex=0.8)
WD.tree.auc <- performance(WDtreepred, 'auc')
tree.auc <- as.numeric(WD.tree.auc@y.values)
tree.auc
WD.bayes.auc <- performance(WDbayespred, 'auc')
bayes.auc <- as.numeric(WD.bayes.auc@y.values)
bayes.auc
WD.bag.auc <- performance(WDbagpred, 'auc')
bag.auc <- as.numeric(WD.bag.auc@y.values)
bag.auc
WD.boost.auc <- performance(WDboostpred, 'auc')
boost.auc <- as.numeric(WD.boost.auc@y.values)
boost.auc
WD.forest.auc <- performance(WDrfpred, 'auc')
forest.auc <- as.numeric(WD.forest.auc@y.values)
forest.auc
output.table <- table(c('Decision Tree', 'Naive Bayes', 'Bagging', 'Boosting',
'Random Forest'), c('Precision', 'Recall', 'Accuracy',
'F1-Score', 'AUC'),
dnn = c('Model', 'Measurement'))
output.table[1, 1] <- bag.accuracy
output.table[1, 2] <- bag.auc
output.table[1, 3] <- bag.f1
output.table[1, 4] <- bag.precision
output.table[1, 5] <- bag.recall
output.table[2, 1] <- boost.accuracy
output.table[2, 2] <- boost.auc
output.table[2, 3] <- boost.f1
output.table[2, 4] <- boost.precision
output.table[2, 5] <- boost.recall
output.table[3, 1] <- tree.accuracy
output.table[3, 2] <- tree.auc
output.table[3, 3] <- tree.f1
output.table[3, 4] <- tree.precision
output.table[3, 5] <- tree.recall
output.table[4, 1] <- bayes.accuracy
output.table[4, 2] <- bayes.auc
output.table[4, 3] <- bayes.f1
output.table[4, 4] <- bayes.precision
output.table[4, 5] <- bayes.recall
output.table[5, 1] <- forest.accuracy
output.table[5, 2] <- forest.auc
output.table[5, 3] <- forest.f1
output.table[5, 4] <- forest.precision
output.table[5, 5] <- forest.recall
output.table
output.table
View(data_summary)
plot(WD.tree); text(WD.tree, pretty = 0)
rpart.plot(WD.bag$trees[[1]], roundint = FALSE)
rpart.plot(WD.boost$trees[[1]], roundint = FALSE)
WD.predtree = predict(WD.tree, WD.test, type = 'class')
t1 = table(Predicted_Class = WD.predtree, Actual_Class = WD.test$Class)
cat('\n#Decision Tree Confusion\n')
t1
cat('\n#Decision Tree Precision\n')
tree.precision <- t1[2, 2]
tree.precision
cat('\n#Decision Tree Recall\n')
tree.recall <- t1[2, 2]/sum(t1[, 2])
tree.recall
cat('\n#Decision Tree Accuracy\n')
tree.accuracy <- sum(t1[2, 2], t1[1, 1])/sum(t1)
tree.accuracy
cat('\n#Decision Tree F1-Score\n')
tree.f1 <- 2 * tree.precision * tree.recall
tree.f1
WD.predbayes = predict(WD.bayes, WD.test)
t2 = table(Predicted_Class = WD.predbayes, Actual_Class = WD.test$Class)
cat('\n#Naive Bayes Confusion\n')
t2
cat('\n#Naive Bayes Precision\n')
bayes.precision <- t2[2, 2]/sum(t2[2, ])
bayes.precision
cat('\n#Naive Bayes Recall\n')
bayes.recall <- t2[2, 2]/sum(t2[, 2])
bayes.recall
cat('\n#Naive Bayes Accuracy\n')
bayes.accuracy <- sum(t2[2, 2], t2[1, 1])/sum(t2)
bayes.accuracy
cat('\n#Naive Bayes F1-Score\n')
bayes.f1 <- (2 * bayes.precision * bayes.recall)/(bayes.precision + bayes.recall)
bayes.f1
WD.predbag <- predict.bagging(WD.bag, WD.test)
cat('\n#Bagging Confusion\n')
WD.predbag$confusion
cat('\n#Bagging Precision\n')
bag.precision <- WD.predbag$confusion[2, 2]/sum(WD.predbag$confusion[2, ])
bag.precision
cat('\n#Bagging Recall\n')
bag.recall <- WD.predbag$confusion[2, 2]/sum(WD.predbag$confusion[, 2])
bag.recall
cat('\n#Bagging Accuracy\n')
bag.accuracy <- sum(WD.predbag$confusion[2, 2], WD.predbag$confusion[1, 1])/
sum(WD.predbag$confusion)
bag.accuracy
cat('\n#Bagging F1-Score\n')
bag.f1 <- (2 * bag.precision * bag.recall)/(bag.precision + bag.recall)
bag.f1
WD.predboost <- predict.boosting(WD.boost, newdata = WD.test)
cat('\n#Boosting Confusion\n')
WD.predboost$confusion
cat('\n#Boosting Precision\n')
boost.precision <- WD.predboost$confusion[2, 2]/sum(WD.predboost$confusion[2, ])
boost.precision
cat('\n#Boosting Recall\n')
boost.recall <- WD.predboost$confusion[2, 2]/sum(WD.predboost$confusion[, 2])
boost.recall
cat('\n#Boosting Accuracy\n')
boost.accuracy <- sum(WD.predboost$confusion[2, 2], WD.predboost$confusion[1, 1])/
sum(WD.predboost$confusion)
boost.accuracy
cat('\n#Boosting F1-Score\n')
boost.f1 <- (2 * boost.precision * boost.recall)/(boost.precision + boost.recall)
boost.f1
WD.predrf <- predict(WD.forest, WD.test)
t3 = table(Predicted_Class = WD.predrf, Actual_Class = WD.test$Class)
cat('\n#Random Forest Confusion\n')
t3
cat('\n#Random Forest Precision\n')
forest.precision <- t3[2, 2]/sum(t3[2, ])
forest.precision
cat('\n#Random Forest Recall\n')
forest.recall <- t3[2, 2]/sum(t3[, 2])
forest.recall
cat('\n#Random Forest Accuracy\n')
forest.accuracy <- sum(t3[2, 2], t3[1, 1])/sum(t3)
forest.accuracy
cat('\n#Random Forest F1-Score\n')
forest.f1 <- (2 * forest.precision * forest.recall)/(forest.precision +
forest.recall)
forest.f1
WD.tree.auc <- performance(WDtreepred, 'auc')
tree.auc <- as.numeric(WD.tree.auc@y.values)
tree.auc
WD.bayes.auc <- performance(WDbayespred, 'auc')
bayes.auc <- as.numeric(WD.bayes.auc@y.values)
bayes.auc
WD.bag.auc <- performance(WDbagpred, 'auc')
bag.auc <- as.numeric(WD.bag.auc@y.values)
bag.auc
WD.boost.auc <- performance(WDboostpred, 'auc')
boost.auc <- as.numeric(WD.boost.auc@y.values)
boost.auc
WD.forest.auc <- performance(WDrfpred, 'auc')
forest.auc <- as.numeric(WD.forest.auc@y.values)
forest.auc
cat('\n#Decision Tree AUC\n')
WD.tree.auc <- performance(WDtreepred, 'auc')
tree.auc <- as.numeric(WD.tree.auc@y.values)
tree.auc
cat('\n#Naive Bayes AUC\n')
WD.bayes.auc <- performance(WDbayespred, 'auc')
bayes.auc <- as.numeric(WD.bayes.auc@y.values)
bayes.auc
cat('\n#Bagging AUC\n')
WD.bag.auc <- performance(WDbagpred, 'auc')
bag.auc <- as.numeric(WD.bag.auc@y.values)
bag.auc
cat('\n#Boosting AUC\n')
WD.boost.auc <- performance(WDboostpred, 'auc')
boost.auc <- as.numeric(WD.boost.auc@y.values)
boost.auc
cat('\n#Random Forest AUC\n')
WD.forest.auc <- performance(WDrfpred, 'auc')
forest.auc <- as.numeric(WD.forest.auc@y.values)
forest.auc
WD.train.nn <- WD.train.new
# Adds indicators
WD.train.nn$Oats = WD.train.nn$Class == 1
WD.train.nn$Other = WD.train.nn$Class == 0
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A16 + A17 + A18 + A19 + A20 + A22 + A25 + A26 + A27 +
A28 + A29, WD.train.nn, hidden = 11, threshold = 0.1,
stepmax = 1e+07)
set.seed(33143188)
WD.train.new <- ovun.sample(Class ~ ., data = WD.train, method = "over",
N = 5996)$data
# Copies the training data into a new data set
WD.train.nn <- WD.train.new
# Adds indicators
WD.train.nn$Oats = WD.train.nn$Class == 1
WD.train.nn$Other = WD.train.nn$Class == 0
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A16 + A17 + A18 + A19 + A20 + A22 + A25 + A26 + A27 +
A28 + A29, WD.train.nn, hidden = 11, threshold = 0.1,
stepmax = 1e+07)
View(WD.nn)
WD.nn$weights
WD.nn
summary(WD.nn)
summary(WD.nn$weights)
WD.nn$weights
# Plots the neural network
plot(WD.nn, rep="best")
# Makes predictions using the test data
WD.prednn <- compute(WD.nn, WD.test[, 1:20])
# Rounds predictions to 0 or 1
WD.prednnr <- round(WD.prednn$net.result, 0)
for(n in 1:length(WD.prednnr)){
if (WD.prednnr[n] < 0){
WD.prednnr[n] <- 0
} else if (WD.prednnr[n] > 1){
WD.prednnr[n] <- 1
}
}
# Creates a data frame of A and B classified as 0 or 1
WD.prednndf <- as.data.frame(as.table(WD.prednnr))
# Removes rows classified 0 to leave only classified 1
WD.prednns <- WD.prednndf[!WD.prednndf$Freq == 0, ]
# Simplify data frame
WD.prednns$Freq <- NULL
colnames(WD.prednns) = c("Obs", "Class")
WD.prednns = WD.prednns[order(WD.prednns$Obs), ]
# Creates a confusion matrix of predicted output against test classes
t5 <- table(observed = WD.test$Class, predicted = WD.prednns$Class)
cat('Artificial Neural Network Confusion')
t5
# Calculates precision
cat('Artificial Neural Network Precision')
nn.precision <- t5[2, 2]/sum(t5[2, ])
nn.precision
# Calculates recall
cat('Artificial Neural Network Recall')
nn.recall <- t5[2, 2]/sum(t5[, 2])
nn.recall
# Calculates accuracy
cat('Artificial Neural Network Accuracy')
nn.accuracy <- sum(t5[2, 2], t5[1, 1])/sum(t5)
nn.accuracy
# Calculates f1-score
cat('Artificial Neural Network F1-Score')
nn.f1 <- (2 * nn.precision * nn.recall)/(nn.precision + nn.recall)
nn.f1
View(WD.train.new)
View(WD.train.new)
# Copies the training data into a new data set
WD.train.nn <- WD.train.new[1:9, 12, 15:21]
# Copies the training data into a new data set
WD.train.nn <- WD.train.new[1:9, 12, 15:21]
# Copies the training data into a new data set
WD.train.nn <- WD.train.new[, c(1:9, 12, 15:21)]
View(WD.train.nn)
# Adds indicators
WD.train.nn$Oats = WD.train.nn$Class == 1
WD.train.nn$Other = WD.train.nn$Class == 0
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A16 + A17 + A18 + A19 + A20 + A22 + A25 + A26 + A27 +
A28 + A29, WD.train.nn, hidden = 11, threshold = 0.1,
stepmax = 1e+07)
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A18 + A22 + A25 + A26 + A27 + A28 + A29, WD.train.nn,
hidden = 11, threshold = 0.1, stepmax = 1e+07)
WD.nn$result.matrix
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A18 + A22 + A25 + A26 + A27 + A28 + A29, WD.train.nn,
hidden = 11, threshold = 0.1, stepmax = 1e+07)
# Plots the neural network
plot(WD.nn, rep="best")
# Makes predictions using the test data
WD.prednn <- compute(WD.nn, WD.test[, 1:20])
# Rounds predictions to 0 or 1
WD.prednnr <- round(WD.prednn$net.result, 0)
for(n in 1:length(WD.prednnr)){
if (WD.prednnr[n] < 0){
WD.prednnr[n] <- 0
} else if (WD.prednnr[n] > 1){
WD.prednnr[n] <- 1
}
}
# Creates a data frame of A and B classified as 0 or 1
WD.prednndf <- as.data.frame(as.table(WD.prednnr))
# Removes rows classified 0 to leave only classified 1
WD.prednns <- WD.prednndf[!WD.prednndf$Freq == 0, ]
# Simplify data frame
WD.prednns$Freq <- NULL
colnames(WD.prednns) = c("Obs", "Class")
WD.prednns = WD.prednns[order(WD.prednns$Obs), ]
# Creates a confusion matrix of predicted output against test classes
t5 <- table(observed = WD.test$Class, predicted = WD.prednns$Class)
cat('Artificial Neural Network Confusion')
t5
# Calculates precision
cat('Artificial Neural Network Precision')
nn.precision <- t5[2, 2]/sum(t5[2, ])
nn.precision
# Calculates recall
cat('Artificial Neural Network Recall')
nn.recall <- t5[2, 2]/sum(t5[, 2])
nn.recall
# Calculates accuracy
cat('Artificial Neural Network Accuracy')
nn.accuracy <- sum(t5[2, 2], t5[1, 1])/sum(t5)
nn.accuracy
# Calculates f1-score
cat('Artificial Neural Network F1-Score')
nn.f1 <- (2 * nn.precision * nn.recall)/(nn.precision + nn.recall)
nn.f1
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A18 + A22 + A25 + A26 + A27 + A28 + A29, WD.train.nn,
hidden = 11, threshold = 0.01, stepmax = 1e+07)
# Plots the neural network
plot(WD.nn, rep="best")
# Makes predictions using the test data
WD.prednn <- compute(WD.nn, WD.test[, 1:20])
# Rounds predictions to 0 or 1
WD.prednnr <- round(WD.prednn$net.result, 0)
for(n in 1:length(WD.prednnr)){
if (WD.prednnr[n] < 0){
WD.prednnr[n] <- 0
} else if (WD.prednnr[n] > 1){
WD.prednnr[n] <- 1
}
}
# Creates a data frame of A and B classified as 0 or 1
WD.prednndf <- as.data.frame(as.table(WD.prednnr))
# Removes rows classified 0 to leave only classified 1
WD.prednns <- WD.prednndf[!WD.prednndf$Freq == 0, ]
# Simplify data frame
WD.prednns$Freq <- NULL
colnames(WD.prednns) = c("Obs", "Class")
WD.prednns = WD.prednns[order(WD.prednns$Obs), ]
# Creates a confusion matrix of predicted output against test classes
t5 <- table(observed = WD.test$Class, predicted = WD.prednns$Class)
cat('Artificial Neural Network Confusion')
t5
# Calculates precision
cat('Artificial Neural Network Precision')
nn.precision <- t5[2, 2]/sum(t5[2, ])
nn.precision
# Calculates recall
cat('Artificial Neural Network Recall')
nn.recall <- t5[2, 2]/sum(t5[, 2])
nn.recall
# Calculates accuracy
cat('Artificial Neural Network Accuracy')
nn.accuracy <- sum(t5[2, 2], t5[1, 1])/sum(t5)
nn.accuracy
# Calculates f1-score
cat('Artificial Neural Network F1-Score')
nn.f1 <- (2 * nn.precision * nn.recall)/(nn.precision + nn.recall)
nn.f1
output.table
# Copies the training data into a new data set
WD.train.nn <- WD.train.new
# Adds indicators
WD.train.nn$Oats = WD.train.nn$Class == 1
WD.train.nn$Other = WD.train.nn$Class == 0
# Fits a neural network model to the training data
WD.nn <- neuralnet(Oats + Other ~ A01 + A02 + A03 + A04 + A07 + A10 + A11 + A14 +
A15 + A16 + A17 + A18 + A19 + A20 + A22 + A25 + A26 + A27 +
A28 + A29, WD.train.nn, hidden = 11, threshold = 0.1,
stepmax = 1e+07)
# Plots the neural network
plot(WD.nn, rep="best")
# Makes predictions using the test data
WD.prednn <- compute(WD.nn, WD.test[, 1:20])
# Rounds predictions to 0 or 1
WD.prednnr <- round(WD.prednn$net.result, 0)
for(n in 1:length(WD.prednnr)){
if (WD.prednnr[n] < 0){
WD.prednnr[n] <- 0
} else if (WD.prednnr[n] > 1){
WD.prednnr[n] <- 1
}
}
# Creates a data frame of A and B classified as 0 or 1
WD.prednndf <- as.data.frame(as.table(WD.prednnr))
# Removes rows classified 0 to leave only classified 1
WD.prednns <- WD.prednndf[!WD.prednndf$Freq == 0, ]
# Simplify data frame
WD.prednns$Freq <- NULL
colnames(WD.prednns) = c("Obs", "Class")
WD.prednns = WD.prednns[order(WD.prednns$Obs), ]
# Creates a confusion matrix of predicted output against test classes
t5 <- table(observed = WD.test$Class, predicted = WD.prednns$Class)
cat('Artificial Neural Network Confusion')
t5
# Calculates precision
cat('Artificial Neural Network Precision')
nn.precision <- t5[2, 2]/sum(t5[2, ])
nn.precision
# Calculates recall
cat('Artificial Neural Network Recall')
nn.recall <- t5[2, 2]/sum(t5[, 2])
nn.recall
# Calculates accuracy
cat('Artificial Neural Network Accuracy')
nn.accuracy <- sum(t5[2, 2], t5[1, 1])/sum(t5)
nn.accuracy
# Calculates f1-score
cat('Artificial Neural Network F1-Score')
nn.f1 <- (2 * nn.precision * nn.recall)/(nn.precision + nn.recall)
nn.f1
output.table$auc
# Creates a table with model as the row and measure of quality as the columns
output.table <- data.frame(c('Decision Tree', 'Naive Bayes', 'Bagging', 'Boosting',
'Random Forest'), c('Precision', 'Recall', 'Accuracy',
'F1-Score'),
dnn = c('Model', 'Measurement'))
# Creates a table with model as the row and measure of quality as the columns
output.table <- data.frame(Model = c('Decision Tree', 'Naive Bayes', 'Bagging', 'Boosting', 'Random Forest'), Precision = c(tree.precision, bayes.precision, bag.precision, boost.precision, forest.precision), Recall = c(tree.recall, bayes.recall, bag.recall, boost.recall, forest.recall), Accuracy = c(tree.accuracy, bayes.accuracy, bag.accuracy, boost.accuracy, forest.accuracy), F1-Score = c(tree.f1, bayes.f1, bag.f1, boost.f1, forest.f1))
# Creates a table with model as the row and measure of quality as the columns
output.table <- data.frame(Model = c('Decision Tree', 'Naive Bayes', 'Bagging', 'Boosting', 'Random Forest'), Precision = c(tree.precision, bayes.precision, bag.precision, boost.precision, forest.precision))
View(output.table)
# Creates a table with model as the row and measure of quality as the columns
output.table <- data.frame(Model = c('Decision Tree', 'Naive Bayes', 'Bagging', 'Boosting', 'Random Forest'), Precision = c(tree.precision, bayes.precision, bag.precision, boost.precision, forest.precision), Recall = c(tree.recall, bayes.recall, bag.recall, boost.recall, forest.recall), Accuracy = c(tree.accuracy, bayes.accuracy, bag.accuracy, boost.accuracy, forest.accuracy), `F1-Score` = c(tree.f1, bayes.f1, bag.f1, boost.f1, forest.f1))
View(output.table)
output.table$AUC <- c(tree.auc, bayes.auc, bag.auc, boost.auc, forest.auc)
output.table
t6
WD.forest.new <- ranger(Class ~ ., data = WD.train.new, num.trees = 10000)
setwd("C:/Users/sophi/OneDrive/Desktop/Monash/IT/FIT3179/Assignment 1")
setwd("C:/Users/sophi/OneDrive/Desktop/Monash/IT/FIT3179/Assignment 2")
setwd("C:/Users/sophi/OneDrive/Desktop/Monash/IT/FIT3179/Assignment 2/FIT3179")
centres <- read.csv("Country_centres.csv")
players <- read.csv("Players_Clubs_Country.csv")
merge(players, centres)
new_data <- merge(players, centres)
View(new_data)
View(new_data)
new_data <- merge(players, centres, all = TRUE)
new_data <- merge(players, centres, all.x = TRUE)
View(new_data)
View(centres)
centres <- read.csv("Country_centres.csv")
new_data <- merge(players, centres)
new_data <- merge(players, centres)[1:9]
View(new_data)
View(centres)
write.csv(new_data, "Country_Data.csv")
